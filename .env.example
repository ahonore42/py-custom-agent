# =============================================================================
# OLLAMA CONFIGURATION
# =============================================================================

# Model to use - see available models with: ollama list
# Examples: glm-4.7-flash:latest, llama3.1:70b, qwen2.5:14b, mistral:7b
OLLAMA_MODEL=glm-4.7-flash:latest

# Ollama API endpoint (usually localhost)
OLLAMA_API_URL=http://localhost:11434/api/generate

# Timeout for LLM requests in seconds
OLLAMA_TIMEOUT=60

# Temperature for LLM responses (0.0 = deterministic, 1.0 = creative)
# Lower values for logic puzzles, higher for creative tasks
OLLAMA_TEMPERATURE=0.7

# =============================================================================
# WEBSOCKET CONFIGURATION
# =============================================================================

# Target WebSocket URI to connect to
WEBSOCKET_URI=ws://example.com/challenge

# WebSocket connection timeout in seconds
WEBSOCKET_TIMEOUT=30

# =============================================================================
# SYSTEM PROMPT CONFIGURATION
# =============================================================================

# Provide either SYSTEM_PROMPT directly or path to a file containing the prompt
# The system prompt instructs the LLM on how to interpret messages and format responses

# Option 1: Direct prompt (for simple cases)
SYSTEM_PROMPT=You are an AI agent solving a challenge. When you receive a message, analyze it and respond with valid JSON. Your response must be properly formatted JSON with no additional text.

# Option 2: Load from file (for complex prompts - uncomment to use)
# SYSTEM_PROMPT_FILE=prompts/my_system_prompt.txt

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Path to log file (will be created automatically)
LOG_FILE=logs/agent_session.log

# Enable console output (in addition to file logging)
ENABLE_CONSOLE_LOG=true

# =============================================================================
# AGENT BEHAVIOR CONFIGURATION
# =============================================================================

# AUTO_MODE=true: Agent responds automatically using AI
# AUTO_MODE=false: Manual mode where you provide JSON responses
AUTO_MODE=true

# Enable automatic reconstruction of fragmented messages
# Set to false if messages are always complete
ENABLE_FRAGMENT_RECONSTRUCTION=true